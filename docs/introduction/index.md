# Introduction to Seldon Core

Seldon Core is an open source platform for deploying machine learning models on Kubernetes. It makes it easy to serve predictions locally and in production.

With Seldon Core you can:
- Deploy models built with any machine learning framework (TensorFlow, PyTorch, scikit-learn, etc).
- Serve predictions using REST, gRPC, Kafka or other protocols.
- Scale model inference on Kubernetes.
- Explain model predictions using integrated algorithms.
- Detect data drift and outliers.
- Chain multiple models into pipelines.
- Reduce costs by packing multiple models into each server.
- Test models locally using Docker before deploying.
Seldon Core handles the full lifecycle of taking models from development to production. It 
provides abstractions that let you focus on the machine learning while it handles the 
deployment and scaling.

The open source core makes Seldon extensible. You can bring your own inference runtimes, 
protocols and components.

To get started, check out the quickstart guides to deploy either locally or on Kubernetes. 
The documentation provides full details on all Seldon Core features.

So if you need to serve machine learning models in production, give Seldon Core a try! 
It provides a robust platform to deploy models at scale.



